{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of Population Predictive Checks to Nonmentioned Bayesian Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Matrix Factorization using the MovieLens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "\n",
    "from numpy.linalg import inv\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "from pyro.infer.mcmc import NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of ratings for each user is 20\n",
      "Minimum number of ratings for each movie is 1\n"
     ]
    }
   ],
   "source": [
    "# Run this when you haven't created a indexed_ratings.csv yet\n",
    "# Data: https://www.kaggle.com/rounakbanik/the-movies-dataset/download\n",
    "ratings_df = pd.read_csv('../data/the-movies-dataset/ratings_small.csv')\n",
    "\n",
    "# Check the minimum number of ratings a user has posted\n",
    "print('Minimum number of ratings for each user is ' + str(ratings_df.groupby('userId').size().min()))\n",
    "print('Minimum number of ratings for each movie is ' + str(ratings_df.groupby('movieId').size().min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use movies that have more than 4 ratings, to have more training data available per movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of ratings for each movie is 5\n"
     ]
    }
   ],
   "source": [
    "# We will only use part of the data that has more than 4 reviews per movie\n",
    "moviecounts = ratings_df.movieId.value_counts()\n",
    "ratings_df = ratings_df[ratings_df.movieId.isin(moviecounts.index[moviecounts.gt(4)])].reset_index(drop=True)\n",
    "print('Minimum number of ratings for each movie is ' + str(ratings_df.groupby('movieId').size().min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this when you DON'T have a train_ratings.csv \n",
    "# Randomly split the data into train and test\n",
    "# Specifically, take 5 random ratings out of each user's rating list (therefore, there will be 5 times # of user ratings in the test set)\n",
    "# For each user id, take out 5 ratings oaut of the dataframe then append them into a new dataframe.\n",
    "# Then take the difference of the two data frames and the difference will be the train data\n",
    "\n",
    "def train_test_split_mf(ratings_df):\n",
    "    for userid in ratings_df.userId.unique() :\n",
    "        if userid == 1:\n",
    "            test_ratings = ratings_df[ratings_df['userId']==userid].sample(5, random_state=0)\n",
    "        else:\n",
    "            test_ratings = test_ratings.append(ratings_df[ratings_df['userId']==userid].sample(5, random_state=0))\n",
    "    train_ratings = pd.concat([ratings_df, test_ratings]).drop_duplicates(keep=False).reset_index(drop=True)\n",
    "    test_ratings = test_ratings.reset_index(drop=True)\n",
    "    print(\"Number of ratings in entire dataset is \"+ str(len(ratings_df)))\n",
    "    print(\"Number of ratings in train dataset is \"+  str(len(train_ratings)))\n",
    "    print(\"Number of ratings in test dataset is \"+ str(len(test_ratings)))\n",
    "    return train_ratings, test_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = train_test_split_mf(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings.to_csv('../data/train_ratings.csv')\n",
    "test_ratings.to_csv('../data/test_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split randomly the data into train and test data. Specifically, we take 5 random ratings out of each user's rating list, creating 5 x (num of users) ratings in the test set. The remaining data becomes the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you have a train_ratings.csv \n",
    "train_ratings = pd.read_csv('../data/train_ratings.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the IDs of the users and movies have gaps in between, we reindex the IDs so that it will be easier to manipulate the user x movie ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you DON'T have a indexed_train_ratings.csv \n",
    "# Reindexing of userId and movieId\n",
    "unique_userId = train_ratings.userId.unique()\n",
    "unique_movieId = train_ratings.movieId.unique()\n",
    "\n",
    "train_ratings['new_user_index'], train_ratings['new_movie_index'] = 0, 0\n",
    "\n",
    "for old_id, new_id in zip(unique_userId, range(len(unique_userId))):\n",
    "    train_ratings['new_user_index'].iloc[train_ratings[train_ratings['userId']==old_id].index.tolist()] = new_id\n",
    "\n",
    "for old_id, new_id in zip(unique_movieId, range(len(unique_movieId))):\n",
    "    train_ratings['new_movie_index'].iloc[train_ratings[train_ratings['movieId']==old_id].index.tolist()] = new_id\n",
    "\n",
    "train_ratings.to_csv('../data/indexed_train_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you have a indexed_train_ratings.csv \n",
    "train_ratings = pd.read_csv('../data/indexed_train_ratings.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of All Ratings (with more than 4 reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of All Ratings (with more than 4 reviews)\n",
    "ratings_df['rating'].value_counts().reindex([0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Train Data Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Train Data Ratings\n",
    "train_ratings['rating'].value_counts().reindex([0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Train Data Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Test Data Ratings\n",
    "test_ratings['rating'].value_counts().reindex([0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Probabilistic Matrix Factorization (PMF) model, and train the model using the train dataset. In our case, PMF 's objective is to fill in the missing values of the movie matrix $M$, by maximizing the following MAP objective function.\n",
    "\n",
    "$L = - \\Sigma_{(i,j)\\in \\Omega}\\frac{1}{2\\sigma^2}(M_{ij}-u_i^Tv_j)^2-\\Sigma_{i=1}^{N_u}\\frac{\\lambda}{2}||u_i||^2-\\Sigma_{j=1}^{N_v}\\frac{\\lambda}{2}||v_j||^2$,\n",
    "\n",
    "where $u_i$ is the $i$th user vector, $v_j$ the $j$th movie vector, $N_u$ the number of users, $N_v$ the number of movies, $\\Omega$ the set of $(i,j)$s that have a rating in the matrix $M$, $\\sigma^2$ the variance of $M_{ij} \\sim N(u_i^Tv_j, \\sigma^2)$. Therefore, each rating in $M$ are assumed to be normally distributed. Also, note that the each user vector $u_i$ and movie vector $v_j$ are initialized from the distribution $N(0, \\lambda^{-1}I)$, where $u$ is a $i \\times k$ matrix and $v$ is a $j \\times k$ matrix.\n",
    "\n",
    "We have implemented the model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of train data using the mean ratings of each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you DON'T have a mean_imputated_ratings.npy\n",
    "\n",
    "imputated_ratings = np.empty((ratings_df.userId.nunique(),ratings_df.movieId.nunique()))\n",
    "\n",
    "for user in range(ratings_df.userId.nunique()):\n",
    "    imputated_ratings[user] = train_ratings[train_ratings.new_user_index == user]['rating'].mean()\n",
    "    for column in train_ratings[train_ratings.new_user_index == user]['new_movie_index']:\n",
    "        imputated_ratings[user, column] = train_ratings[(train_ratings.new_user_index == user)&(train_ratings.new_movie_index == column)]['rating']   \n",
    "\n",
    "np.save('../data/mean_imputated_ratings.npy', imputated_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you have a imputated_ratings.npy\n",
    "imputated_ratings = np.load('../data/mean_imputated_ratings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of train data using zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you DON'T have a zero_imputated_ratings.npy\n",
    "\n",
    "imputated_ratings = np.empty((ratings_df.userId.nunique(),ratings_df.movieId.nunique()))\n",
    "\n",
    "for user in range(ratings_df.userId.nunique()):\n",
    "    imputated_ratings[user] = 0\n",
    "    for column in train_ratings[train_ratings.new_user_index == user]['new_movie_index']:\n",
    "        imputated_ratings[user, column] = train_ratings[(train_ratings.new_user_index == user)&(train_ratings.new_movie_index == column)]['rating']  \n",
    "\n",
    "np.save('../data/zero_imputated_ratings.npy', imputated_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when you have a zero_imputated_ratings.npy\n",
    "imputated_ratings = np.load('../data/zero_imputated_ratings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Probabilistic Matrix Factorization using Pyro\n",
    "# (To use Pyro, need data that has missing values imputated with the mean rating of each user)\n",
    "def model(data, k=10):\n",
    "    with pyro.plate('users', ratings_df.userId.nunique()):\n",
    "        u = pyro.sample('u', dist.MultivariateNormal(torch.zeros(k), 1*torch.eye(k)))\n",
    "    with pyro.plate('movies', ratings_df.movieId.nunique()):\n",
    "        v = pyro.sample('v', dist.MultivariateNormal(torch.zeros(k), 1*torch.eye(k)))\n",
    "    r = pyro.sample(\"obs\", dist.Normal(torch.mm(u,v.T), 1), obs=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "kernel = NUTS(model, step_size=1e-4) \n",
    "mcmc = MCMC(kernel, num_samples=5, warmup_steps=25)\n",
    "mcmc.run(torch.tensor(imputated_ratings))\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(posterior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pmf_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(posterior_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(torch.mm(posterior_samples['u'][i,:,:].T, posterior_samples['v'][i,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor =  torch.mm(posterior_samples['u'][0,:,:], posterior_samples['v'][0,:,:].T)\n",
    "for i in range(1,5):\n",
    "    tensor += torch.mm(posterior_samples['u'][i,:,:], posterior_samples['v'][i,:,:].T)\n",
    "tensor = tensor/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_estimate_ratings = (tensor* 2).round()/ 2\n",
    "rounded_estimate_ratings[rounded_estimate_ratings>5.0] = 5.0\n",
    "rounded_estimate_ratings[rounded_estimate_ratings<0.5] = 0.5\n",
    "print(rounded_estimate_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_estimate_ratings_unique = rounded_estimate_ratings.unique(sorted=True)\n",
    "rounded_estimate_ratings_counts = torch.stack([(rounded_estimate_ratings==rounded_estimate).sum() for  rounded_estimate in rounded_estimate_ratings_unique])\n",
    "rounded_estimate_ratings_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(rounded_estimate_ratings_unique, rounded_estimate_ratings_counts, width=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the predicted data is distributed normally, due to the fact that the PMF assumes a normal distribution on its ratings, and the variance of each ratings are the same. Therefore, there seems to be space for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PMF using Pyro\n",
    "# Reference https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf\n",
    "# (To use Pyro, need data that has missing values imputated with the mean rating of each user)\n",
    "def model(data, k=10):\n",
    "    with pyro.plate('users', ratings_df.userId.nunique()):\n",
    "        with pyro.plate('user_k', k):\n",
    "            p_u_mu = pyro.sample('p_u_mu', dist.Normal(0, 1))\n",
    "            p_u_std = pyro.sample('p_u_std', dist.InverseGamma(1,1))\n",
    "            u = pyro.sample('u', dist.Normal(p_u_mu, p_u_std))\n",
    "    with pyro.plate('movies', ratings_df.movieId.nunique()):\n",
    "        with pyro.plate('movie_k', k):\n",
    "            p_v_mu = pyro.sample('p_v_mu', dist.Normal(0, 1))\n",
    "            p_v_std = pyro.sample('p_v_std', dist.InverseGamma(1,1))\n",
    "            v = pyro.sample('v', dist.Normal(p_v_mu, p_v_std))\n",
    "    pyro.sample(\"obs\", dist.Normal(torch.mm(u.T,v), 1), obs=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup:   3%|â–         | 6/200 [00:16,  2.66s/it, step size=1.59e-02, acc. prob=0.611]"
     ]
    }
   ],
   "source": [
    "pyro.set_rng_seed(2)\n",
    "kernel = NUTS(model, step_size=1e-4) \n",
    "mcmc = MCMC(kernel, num_samples=100, warmup_steps=100)\n",
    "mcmc.run(torch.tensor(imputated_ratings))\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POP-PC\n",
    "# Cross validation POP-PC\n",
    "# Randomly remove ratings from train data, and keep possession of them to check later on \n",
    "# Train the model, then compare the entire data distribution and the ratings distributions of each user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Posterior Predictive Check (PPC) to criticize this model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPC \n",
    "# get distribution of ratings from the entire data, and for each user\n",
    "# get distribution of ratings from the predicted entire data, and predictions for each user\n",
    "# check if the discrepancies are close to each other\n",
    "\n",
    "# Discrepancies of original data\n",
    "# Max and min ratings are trivially the same\n",
    "mean_rating = train_ratings['rating'].mean()\n",
    "median_rating = train_ratings['rating'].median()\n",
    "first_q_rating = np.quantile(train_ratings['rating'],0.25)\n",
    "third_q_rating = np.quantile(train_ratings['rating'],0.75)\n",
    "\n",
    "# Discrepancies of trained_data\n",
    "pred_mean_rating = torch.mean(rounded_estimate_ratings)\n",
    "pred_median_rating = torch.median(rounded_estimate_ratings)\n",
    "pred_first_q_rating = np.quantile(rounded_estimate_ratings.numpy(),0.25)\n",
    "pred_third_q_rating = np.quantile(rounded_estimate_ratings.numpy(),0.75)\n",
    "\n",
    "print('Discrepancy of mean ratings: ' + str(mean_rating - pred_mean_rating))\n",
    "print('Discrepancy of median ratings: ' + str(median_rating - pred_median_rating))\n",
    "print('Discrepancy of first_q ratings: ' + str(first_q_rating - pred_first_q_rating))\n",
    "print('Discrepancy of third_q ratings: ' + str(third_q_rating - pred_third_q_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared the distribution of the train data ratings with the predicted $M^{\\prime}$ ratings. The discrepancies of the distributions are big considering that the values of ratings are between 0.5~5, which means that the PMF model is not performing well. This is trivial when you compare the two distributions visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_estimate_ratings_unique = rounded_estimate_ratings.unique(sorted=True)\n",
    "rounded_estimate_ratings_counts = torch.stack([(rounded_estimate_ratings==rounded_estimate).sum() for  rounded_estimate in rounded_estimate_ratings_unique])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of ratings in the predicted $M^{\\prime}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(rounded_estimate_ratings_unique, rounded_estimate_ratings_counts, width=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Train Data Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ratings['rating'].value_counts().reindex([0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0]).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Population Predictive Checks https://arxiv.org/abs/1908.00882\n",
    "- Probabilistic Matrix Factorization https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n",
    "- Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf\n",
    "- Scalable Recommendation with Poisson Factorization https://arxiv.org/pdf/1311.1704.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Pyro Implementation of Probabilistic Matrix Factorization (Could achieve PMF without using imputed values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://sandipanweb.wordpress.com/2017/04/04/probabilistic-matrix-factorization-with-a-generative-model-in-python/\n",
    "# Implementation of Probabilistic Matrix Factorization from the above reference\n",
    "class PMF:\n",
    "    def __init__(self, k, prior_mean=0, prior_std=1, epochs=20):\n",
    "        self.k = k\n",
    "        self.prior_mean = prior_mean\n",
    "        self.prior_std = prior_std\n",
    "        self.epochs = epochs\n",
    "        self.u = None\n",
    "        self.v = None\n",
    "        self.M_prime = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        ratings = data\n",
    "        self.I = ratings.userId.nunique()\n",
    "        self.J = ratings.movieId.nunique()\n",
    "        self.u = pyro.sample(\"user_factor\", dist.Normal(self.prior_mean, self.prior_std), sample_shape=torch.Size([self.I,self.k]))\n",
    "        self.v = pyro.sample(\"movie_factor\", dist.Normal(self.prior_mean, self.prior_std), sample_shape=torch.Size([self.J,self.k]))\n",
    "        self.data_var = ratings['rating'].var(axis=0)\n",
    "        \n",
    "        user_to_movie_dict = {}\n",
    "        movie_to_user_dict = {}\n",
    "\n",
    "        for i in range(self.I):\n",
    "            user_to_movie_dict[i] = ratings[ratings['new_user_index']==i]['new_movie_index'].tolist()\n",
    "        for j in range(self.J):\n",
    "            movie_to_user_dict[j] = ratings[ratings['new_movie_index']==j]['new_user_index'].tolist()\n",
    "            \n",
    "        for epoch_num in range(self.epochs):\n",
    "            # Update of all u_i: Took 110.703 seconds on Yuki's cp\n",
    "            for i in range(len(self.u)):\n",
    "                v_sum = 0\n",
    "                mv_sum = 0\n",
    "                for j in user_to_movie_dict[i]:\n",
    "                    v_sum += np.outer(self.v[j],self.v[j])\n",
    "                    mv_sum += float(ratings[(ratings['new_user_index']==i) & (ratings['new_movie_index']==j)].rating) * self.v[j]\n",
    "                self.u[i] = torch.from_numpy(np.dot(inv(self.prior_std * self.data_var * np.identity(self.k) + v_sum), mv_sum))\n",
    "\n",
    "            # Update of all v_j: Took 106.218 seconds on Yuki's cp\n",
    "            for j in range(len(self.v)):\n",
    "                u_sum = 0\n",
    "                mu_sum = 0\n",
    "                for i in movie_to_user_dict[j]:\n",
    "                    u_sum += np.outer(self.u[i],self.u[i])\n",
    "                    mu_sum += float(ratings[(ratings['new_user_index']==i) & (ratings['new_movie_index']==j)].rating) * self.u[i]\n",
    "                self.v[j] = torch.from_numpy(np.dot(inv(self.prior_std * self.data_var * np.identity(self.k) + u_sum), mu_sum))\n",
    "\n",
    "            self.M_prime = torch.mm(self.u,self.v.T)\n",
    "            error = 0\n",
    "            for i in range(len(self.u)):\n",
    "                for j in user_to_movie_dict[i]:\n",
    "                    error += (float(ratings[(ratings['new_user_index']==i) & (ratings['new_movie_index']==j)].rating) - self.M_prime[i][j])**2\n",
    "            print('MSE of Epoch ' + str(epoch_num) + ': ' + str(float(error/len(ratings))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the PMF using $k=10$ and 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMF_self = PMF(10)\n",
    "PMF_self.fit(train_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the predicted $M^{\\prime}$ with the filled missing values, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_ratings = PMF_self.M_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the estimated ratings to the closest .5 rating between 0.5 and 5\n",
    "rounded_estimate_ratings = (estimate_ratings * 2).round()/ 2\n",
    "rounded_estimate_ratings[rounded_estimate_ratings>5.0] = 5.0\n",
    "rounded_estimate_ratings[rounded_estimate_ratings<0.5] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "with open('10k20epochPMF.pkl', 'wb') as output:\n",
    "    pickle.dump(model_k10_e20, output, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
